"""–£ —Ç–µ–±—è —Å–µ–π—á–∞—Å –≤—Å—ë –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ —á–µ—Ä–µ–∑ find_all, –∏ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–æ –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å –Ω–∞ select, —Ç–æ –ª–æ–≥–∏–∫–∞ –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–∞ –∂–µ, –ø—Ä–æ—Å—Ç–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –±—É–¥—É—Ç CSS-—Å—Ç–∏–ª—è–º–∏.

–ü–æ–¥—Å–∫–∞–∑–∫–∏, –∫–∞–∫ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –Ω–∞ select

–í–º–µ—Å—Ç–æ —Ç—Ä—ë—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ (quotes, authors, tags) –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, div.quote.
–¢–æ–≥–¥–∞ –≤ –∫–∞–∂–¥–æ–º block —Å—Ä–∞–∑—É –µ—Å—Ç—å –∏ —Ç–µ–∫—Å—Ç —Ü–∏—Ç–∞—Ç—ã, –∏ –∞–≤—Ç–æ—Ä, –∏ —Ç–µ–≥–∏. –≠—Ç–æ –∏–∑–±–∞–≤–∏—Ç —Ç–µ–±—è –æ—Ç –ø—É—Ç–∞–Ω–∏—Ü—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏.

–ò–∑ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞:

–¶–∏—Ç–∞—Ç–∞: block.select_one("span.text").get_text(strip=True)

–ê–≤—Ç–æ—Ä: block.select_one("small.author").get_text(strip=True)

–¢–µ–≥–∏: [t.get_text(strip=True) for t in block.select("div.tags a.tag")]

–í data.append(...) —É–∂–µ –∫–ª–∞–¥—ë—à—å —Å–ª–æ–≤–∞—Ä—å —Å —Ç—Ä–µ–º—è –∫–ª—é—á–∞–º–∏.

üëâ –¢.–µ. –≤ –≥–æ–ª–æ–≤–µ —É —Ç–µ–±—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–∫–∞—è:

–ù–∞–π—Ç–∏ –≤—Å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: soup.select("div.quote")

–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –¥–æ—Å—Ç–∞—Ç—å span.text, small.author, div.tags a.tag

–°–æ–±—Ä–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å, –¥–æ–±–∞–≤–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫
"""


import json
from bs4 import BeautifulSoup
import requests

BASE_URL = 'https://quotes.toscrape.com/'

html_doc = requests.get(BASE_URL)
soup = BeautifulSoup(html_doc.text, 'lxml')
#authors_link = []
base = soup.select('div.quote')


def quotes():
    data = []
    for el in base:
        quote = el.select_one('span.text').get_text(strip=True)
        author = el.select_one('small.author').get_text(strip=True)
        tags = [tag.get_text(strip=True) for tag in el.select('div.tags a.tag')]
        data.append({'tags': tags, 'author': author, 'quote': quote})

    with open('quotes_select_method.json', 'w')as q:
        json.dump(data, q, indent=4, ensure_ascii=False)

def authors():
    authors_data = []
    for el in base:
        author_link = el.select_one('a').get('href')
        #authors_link.append(author_link)
        author_doc = requests.get(BASE_URL+author_link)
        author_soup = BeautifulSoup(author_doc.text, 'lxml')
        author_name = author_soup.select_one('div.author-details h3.author-title').get_text(strip=True)
        author_born_date = author_soup.select_one('span.author-born-date').get_text(strip=True)
        author_born_location = author_soup.select_one('span.author-born-location').get_text(strip=True)
        author_description = author_soup.select_one('div.author-description').get_text(strip=True)
        authors_data.append({'fullname': author_name, 'born_date': author_born_date, 'born_location': author_born_location, 'description': author_description})
        
    with open('author_select_method.json', 'w') as a:
        json.dump(authors_data, a, ensure_ascii=False, indent=4)

quotes()
authors()


